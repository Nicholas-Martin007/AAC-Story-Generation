{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a0760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/chatbot_beta/nic-learn/skripsi_nic/nic_env/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/dev/chatbot_beta/nic-learn/skripsi_nic/nic_env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('./'))\n",
    "from config import *\n",
    "\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    PeftModelForCausalLM,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_from_disk\n",
    "from evaluate import load as load_metric\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4717db48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_PATH,\n",
    ")\n",
    "\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        'additional_special_tokens': [\n",
    "            '<|PER|>',\n",
    "            '<|PER_1|>',\n",
    "            '<|PER_2|>',\n",
    "            '<|PER_3|>',\n",
    "            '<|PER_4|>',\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '<|PAD|>'})\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\n",
    "        '<|PAD|>'\n",
    "    )\n",
    "\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4de66c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f'{MODEL_PATH}')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "    model,\n",
    "    f'{MODEL_PATH}-qlora',\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "# model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     f\"{MODEL_PATH}-qlora\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     device_map=DEVICE\n",
    "# )\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model = merged_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98041f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Jul 2025\n",
      "\n",
      "Tulis sebuah kisah sosial dari kartu-kartu yang diberikan<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "['Ayah', 'loncat', 'kuda','main', 'dengan']<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ketika masih kecil, <|PER|> suka melihat ayahnya bermain dengan kuda. Dia sering melihat ayahnya berdiri di atas kuda, berlatih teknik berlari dan berayun. Suatu hari, ayahnya meminta <|PER|> untuk mencoba berada di atas kuda. Awalnya, <|PER|> merasa takut, tapi ayahnya memberikan motivasi yang kuat. Ia mengatakan bahwa tidak ada yang salah untuk mencoba, dan bahwa dia akan ada di sampingnya selalu. Dengan tekad yang kuat, <|PER|> akhirnya berhasil naik ke atas kuda. Mereka berdua berlatih bersama, dan <|PER|> mulai menyukai kegiatan tersebut.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': r'Tulis sebuah kisah sosial dari kartu-kartu yang diberikan',\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                'Ayah',\n",
    "                'loncat',\n",
    "                'kuda',\n",
    "                'main',\n",
    "                'dengan',\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    ").input_ids.to(DEVICE)\n",
    "\n",
    "\n",
    "output = merged_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.9,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "story = tokenizer.decode(output[0])\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baadd1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|PER_1|>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"PRESON\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.tokenize('<|PER_1|>'))\n",
    "\n",
    "'\"PRESON\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30b10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
